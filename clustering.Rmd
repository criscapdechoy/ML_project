---
  
  author: "Cristina Capdevila"
---


Clustering!
```{r echo=FALSE, message=FALSE, warning=FALSE}
rm(mydataset,X)
library(dplyr)
library (ggplot2)
library(FactoMineR)
library(factoextra)
library("matrixStats")
library("ggplot2")
library("gridExtra")
library("ggpubr")
library("DMwR")
library("plsdepot")
library("ggplot2")
library("ggpubr")
library ("cclust")
library("fpc")

nutrition <- c(
  "fat_100g",
  "carbohydrates_100g",
  "sugars_100g",
  "proteins_100g",
  "salt_100g",
  "fiber_100g"
)

#remove factor and responsive vble
set.seed(1876)
mydataset <- read.csv("final_data.csv")

test.index = sample(1:nrow(mydataset), nrow(mydataset)*0.01)

data.nograde <- mydataset[c("nutriscore_score","nova_group",nutrition, "additives_n")]
data.test = data.nograde[test.index,]
data.train = data.nograde[-test.index,]
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#PCA con funcion
res.pca <- prcomp(data.test, center = TRUE, scale. = TRUE)
summary(res.pca)

fviz_eig(res.pca)
# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
eigenvalues <- eig.val$eigenvalue
kaiser.rule.lim <- mean(eigenvalues)
sign.comp <- which(eig.val$eigenvalue > kaiser.rule.lim) #we have 3 significant dimentions
print(sign.comp)
## print same plots in previous pca for sing comp analysys
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### hc clustering

res.ind <- get_pca_ind(res.pca)
Psi <- res.ind$coord[,sign.comp]
d <- dist(Psi, method = "euclidean")
h.clust <- hclust(d, method="ward.D2")
plot(h.clust)

ini = length(h.clust$height)-20
barplot(h.clust$height[ini:length(h.clust$height)],title="20 Larger tree heights")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
##consolidation operation

library(ggthemes)
ind.clusters = cutree(hc, 5)
table(ind.clusters)

da = list(data=Psi, cluster=ind.clusters)

centroids = aggregate(Psi, list(ind.clusters), mean)

clust.cent <- aggregate(Psi,list(ind.clusters),mean)[,2:4]
Bss <- sum(rowSums(clust.cent^2)*as.numeric(table(ind.clusters)))
Tss <- sum(rowSums(Psi^2))
Ib <- 100*Bss/Tss
print(paste0("Inertia between clusters is ", round(Ib, digits=3), "%"))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
k.means <- kmeans(Psi,centers=clust.cent)

###catdes

catdes(cbind(as.factor(k.means$cluster), data.test),1)

plot(Psi[,1],Psi[,2],type="n",main="Clustering with consolidation operation")
text(Psi[,1],Psi[,2],col=k5$cluster,cex = 0.6)
abline(h=0,v=0,col="gray")
legend("bottomleft",c("c1","c2","c3","c4", "c5"),pch=20,col=c(1:5))

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
#define number of classes
num.clust <- 5
ind.clusters <- cutree(h.clust, k=num.clust)

table(ind.clusters) #classes and nr of elements for each

da = list(data=Psi, cluster=ind.clusters)
plot <-fviz_cluster(da,ggtheme = theme_minimal())
plot


```


```{r echo=FALSE, message=FALSE, warning=FALSE}
#  5  Compute the Calinski-Harabassz index 
Wss <- sum(k.means$withinss)                        

before_conso_index =  round(calinhara(da$data,da$cluster),digits=2)
before_conso_index
 
  
after_conso_index = round(calinhara(Psi,k.means$cluster),digits=2)
after_conso_index


#CH.3 <- clustIndex(Psi,int.clust, index="calinski")
CH_after= (Bss/Wss)
CH_after

CH_before=Bss/(Bss+Tss)
CH_before
#calculate it with and without kmeans


#the result is improved with calinski-Harabassz


# 6   Using the function catdes interpret and name the obtained clusters
#and represent them in the first factorial display. 
desc <- catdes(as.factor(k.means$cluster),1)
desc$quanti

#  representation of clusters in first factorial plane

kmeans.cent <- k.means$centers
kmeans.cent

first_factorial <- fviz_cluster(k.means, data=Psi, k = num.clust, rect = TRUE, main = "Hclust ward",ggtheme = theme_minimal())
first_factorial




```
