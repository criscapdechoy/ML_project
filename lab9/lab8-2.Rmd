---
title: 'LAB 8: Ensemble methods'
subtitle: 'Example 2: Spam mail detection with extreme gradient boosting (xgboost)'
author: "Marta Arias"
date: "April 2020"
output:
  html_notebook: default
  pdf_document: default
editor_options: 
  chunk_output_type: inline
---

In this second example script illustrating the use of (extreme) gradient boosting, we will continue to work with the same spam email dataset. So we load, and pre-process the data as in the random forest example:

```{r}
library(kernlab)  
data(spam)

# log-transform count columns
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))

spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]

george.vars <- 25:28
spam2 <- spam2[,-george.vars]

moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
```



As it is customary, we start by splitting the available data into learning and test sets, selecting in this case randomly 2/3 and 1/3 of the data for final testing.


```{r}
set.seed(4321)

N <- nrow(spam3)                                                                                              
learn <- sample(1:N, round(2/3*N))
nlearn <- length(learn)

library(xgboost)

# prepare data for xgboost
labels <- as.integer(spam3$type)-1 # 0 for nonspam, 1 for spam
data <- as.matrix(spam3[,-50])

dtrain = xgb.DMatrix(data=data[learn,], label=labels[learn])
dtest = xgb.DMatrix(data=data[-learn,], label=labels[-learn])
```

### 1. Building a default boosting model

So, let's build a basic boosting classifier with default parameters and using 100 trees:

```{r}
model.100trees <- xgb.train(data = dtrain, objective = "binary:logistic", nrounds=100)
```

Now, let us evaluate it on the test set; since this is a procedure that we will repeat with successive models we will be building, let's make a function out of it:

```{r}
evaluate_classifier <- function (msg, model, data, classes) {
  pred.class = round(predict(model, data))

  harm <- function (a,b) { 2/(1/a+1/b) }   # harmonic mean

  ct <- table(Truth=classes, Pred=pred.class)               # confusion matrix on test set
  error1 <- prop.table(ct, 1)[1,2]                          # test error rates for 1st class
  error2 <- prop.table(ct, 1)[2,1]                          # test error rates for 2nd class
  test_error <- round(100*(1-sum(diag(ct))/sum(ct)),2)      # test error rate
  test_F1 <- harm(prop.table(ct,1)[1,1], prop.table(ct,1)[2,2])   # F1 measure
  
  # print stuff
  print(msg)
  cat("error on nonspam:", error1*100, "error on spam:", error2*100, "\n")
  cat("error rate:", test_error, "\n")
  cat("F1 score:", test_F1*100, "\n")
}
```

Let us evaluate our xgboost model:

```{r}
evaluate_classifier("model with no tuning (with 100 trees)",
                    model.100trees, dtest, labels[-learn])
```

Mmmh not bad! Since this algorithm has many parameters, we may try to optimize them with cross-validation. 

***

### 2. Tuning parameters

The package `xgboost` has many clever tricks controlled by hyper-parameters. We only cover the main ones here for clarity:

- `nrounds`: basically, how many individual trees we add to the final model. In contrast to bagging, in boosting trees are inter-dependent, mainly the next tree to be added depends on the trees added so far, and therefore this iterative procedure cannot be parallelized. In a way, we are improving the ensemble at each iteration by adding a new tree that captures some stuff that previous trees did not.
- `max_depth`: maximum depth of trees in the ensemble, higher values correspond to more complex weak learners (default is 6)
- `eta`: learning rate or shrinkage; this determines the coefficient used when adding new trees to the ensemble. Smaller values prevent overfitting but make the learning process slower so more rounds may be required. (default is 0.3)

The first thing we are going to do is tune the number of trees, i.e., `nrounds`, in the final boosting model. For that we use the built-in cross-validation procedure that is luckily implemented in the package. Moreover, in order to save computation time we are going to use the **early stopping** feature provided, which in essence stops adding new trees if after a few iterations validation performance has not improved. The rest of paramters are left as default; the code uses 5-fold cross-validation and we let nrounds potentially run until 1000 unless stopping early.

During the cross-validation, we use a different metric that is related to the $F_1$ score to take into account possible class imbalance.

```{r}
cv.nround = 1000
cv.nfold = 5
mdcv <- xgb.cv(data=dtrain, objective="binary:logistic", nfold=cv.nfold, 
               nrounds=cv.nround, nthread=4,
               metrics = list("aucpr"),
               early_stopping_rounds=50, maximize=TRUE,
               print_every_n = 10)

max_aucpr = max(mdcv$evaluation_log$test_aucpr_mean)
max_aucpr_index = which.max(mdcv$evaluation_log$test_aucpr_mean)
```

So, in this case the best nr of trees found was `r max_aucpr_index` with cross-validation *area under the precison-recall curve* of `r max_aucpr`.

We retrain on whole training set and evaluate the resulting classifier:

```{r}
xgb.tuned1 <- xgb.train(data=dtrain, nrounds=max_aucpr_index)
evaluate_classifier(
  sprintf("model with tuned ensemble size to %d", max_aucpr_index),
                    xgb.tuned1, dtest, labels[-learn])
```


But let us continue with this idea and tune the two remaining parameters: `eta` and `max_depth`. Instead of using all possible combination of values, we are going to do this search in a randomized fashion. So, we iterate 50 times chosing random values for these two parameters and then optimize nrounds as well as we did before. We will keep track of the best values seen so far while doing the iterated cross-validations. 

Warning: this may take a long time... please be patient... or run fewer iterations! (the search may not explore enough, then).

```{r echo=T, results='hide'}
set.seed(1234)

best_param = list()
best_aucpr = 0
best_aucpr_index = 0

for (iter in 1:50) {
    param <- list(
          max_depth = sample(3:6, 1),
          eta = runif(1, .01, .3)
          )
    cat("**iter:", iter, 
        "with eta=", param$eta, "max_depth=", param$max_depth, "\n")
    cv.nround = 1000
    cv.nfold = 5
    mdcv <- xgb.cv(data=dtrain, params = param, print_every_n = 10,
                   nfold=cv.nfold, nrounds=cv.nround, nthread=4,
                    early_stopping_rounds=50, maximize=TRUE,
                   metrics = "aucpr",
                   objective="binary:logistic")

    max_aucpr = max(mdcv$evaluation_log$test_aucpr_mean)
    max_aucpr_index = which.max(mdcv$evaluation_log$test_aucpr_mean)

    if (max_aucpr > best_aucpr) {
        best_aucpr = max_aucpr
        best_aucpr_index = max_aucpr_index
        best_param = param
    }
}
```

The best parameters found by the cross-validation are:
```{r}
print(best_param)
print(best_aucpr_index)
```


Finally, we retrain on the whole training set with best parameters found and evaluate it:

```{r}
xgb.tuned2 <- xgb.train(data=dtrain, params=best_param, objective = "binary:logistic", nrounds=best_aucpr_index)
msg <- sprintf("tuned model: ensemble size=%d, eta=%f, max_depth=%d", best_aucpr_index,
        best_param$eta, best_param$max_depth)
evaluate_classifier(msg, xgb.tuned2, dtest, labels[-learn])
```


### 3. Variable importance

It is also possible to extract a variable importance assessment using boosting (basically using the same variable importance measure as in random forests, and then using the linear coefficients used in the boosted ensemble to mix those importances). One could use this as the basis for feature selection in other future models, for example. And it is useful to understand the model and therefore, the data. In the following code, we just show the most important 20 variables but you could of course select more or all of them.

```{r}
mat <- xgb.importance (feature_names = colnames(dtrain), model = xgb.tuned2)
xgb.plot.importance (importance_matrix = mat[1:20]) 
```

### 4. Take-home lessons from this script

- How to build boosting ensembles for classification using extreme gradient boosting
- How to tune ensemble size using cross-validation and early stopping
- How to tune parameters using cross-validation
- How to extract variable importance information
